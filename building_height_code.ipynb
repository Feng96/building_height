{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a minimal reproducible example for the state \"AK\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import gzip\n",
    "import json\n",
    "from shapely.geometry import shape\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import zipfile\n",
    "import fiona\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEMA and MS data downloaded and extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Download datasets and needed files\n",
    "\n",
    "def download_and_save(url, dest_path):\n",
    "    \"\"\"Download a file from a given URL and save it locally.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with open(dest_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192): \n",
    "            file.write(chunk)\n",
    "\n",
    "# Extract state code from the ZIP URL\n",
    "zip_url = 'https://disasters.geoplatform.gov/publicdata/Partners/ORNL/USA_Structures/Alaska/Deliverable20230728AK.zip'\n",
    "state_code = zip_url.split('Deliverable')[1][8:10]  # Extracts 'AK' from the provided URL\n",
    "\n",
    "# Define directories\n",
    "base_dir = './downloads'\n",
    "fema_dir = os.path.join(base_dir, 'FEMA')\n",
    "state_folder = os.path.join(fema_dir, state_code)\n",
    "zip_folder = os.path.join(state_folder, 'zip_files')\n",
    "extract_folder = os.path.join(state_folder, 'extracted_files')\n",
    "geojson_folder = os.path.join(base_dir, 'geojson_files')\n",
    "csv_folder = os.path.join(base_dir, 'csv_files')\n",
    "\n",
    "geojson_url = 'https://gist.githubusercontent.com/Feng96/bf831f5cf972d24455a25128ed8b3636/raw/f05772ff636c790f6ebc0e4eb5aa6f7b669f4ea9/data_link.geojson'\n",
    "csv_url = 'https://minedbuildings.blob.core.windows.net/global-buildings/dataset-links.csv'\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(zip_folder, exist_ok=True)\n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "os.makedirs(geojson_folder, exist_ok=True)\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "\n",
    "zip_dest_path = os.path.join(zip_folder, f'Deliverable{state_code}.zip')\n",
    "geojson_dest_path = os.path.join(geojson_folder, 'code.geojson')\n",
    "csv_dest_path = os.path.join(csv_folder, 'dataset-links.csv')\n",
    "\n",
    "# Download and save files\n",
    "download_and_save(zip_url, zip_dest_path) # FEMA US Structures ZIP file\n",
    "download_and_save(geojson_url, geojson_dest_path) # MS buildings with height coverage GeoJSON file\n",
    "download_and_save(csv_url, csv_dest_path) # CSV file with links to the MS footprint data\n",
    "\n",
    "# Extract ZIP content\n",
    "with zipfile.ZipFile(zip_dest_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "\n",
    "print('FEMA and MS data downloaded and extracted successfully!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(csv_folder, 'dataset-links.csv'))\n",
    "\n",
    "gdf = gpd.read_file(os.path.join(geojson_folder, 'code.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_url = 'https://raw.githubusercontent.com/giswqs/data/main/us/us_states.csv'\n",
    "states = pd.read_csv(states_url)\n",
    "print(states.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing state: Alaska (AK)\n",
      "Downloading files for state: Alaska (AK)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:21<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files for state: Alaska (AK)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [02:05<00:00, 10.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available layers: ['AK_Structures']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utkge\\miniconda3\\envs\\gee\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3382: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving shp file for state: Alaska (AK)\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "output_folder = os.path.join(base_dir, 'joined')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#loop through each state\n",
    "state_name = \"Alaska\"\n",
    "state = state_code\n",
    "gdf_state = gdf[gdf['STUSPS'] == state].copy()  # Create a copy\n",
    "\n",
    "print(f\"Processing state: {state_name} ({state})\")\n",
    "\n",
    "# Convert 'field_1' to 'QuadKey' as a string\n",
    "gdf_state.loc[:, 'QuadKey'] = gdf_state['field_1'].apply(lambda x: str(int(str(x)[1:9])))\n",
    "\n",
    "# Ensure 'QuadKey' in df is also a string\n",
    "df['QuadKey'] = df['QuadKey'].astype(str)\n",
    "\n",
    "# Filter out rows where 'QuadKey' does not match with 'QuadKey' in gdf_state\n",
    "df_state = df[df['QuadKey'].isin(gdf_state['QuadKey'])]\n",
    "\n",
    "print(f\"Downloading files for state: {state_name} ({state})\")\n",
    "# Create directory to save downloaded files\n",
    "os.makedirs(os.path.join(base_dir, f'downloaded_files/{state}'), exist_ok=True)\n",
    "\n",
    "# Iterate over all rows in the filtered dataframe\n",
    "for idx, row in tqdm(df_state.iterrows(), total=df_state.shape[0]):\n",
    "        # Get the URL\n",
    "    url = row['Url']\n",
    "\n",
    "    # Make a GET request to the URL\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Get the file name by joining the 'downloaded_files' with the basename of the URL\n",
    "        file_name = os.path.join(base_dir, f'downloaded_files/{state}', f\"{state}_{idx}.csv.gz\")\n",
    "\n",
    "        # Open the file in write mode\n",
    "        with open(file_name, 'wb') as file:\n",
    "            # Write the content of the request to the file\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                file.write(chunk)\n",
    "    else:\n",
    "        print(f\"Failed to download file from url {url}\")\n",
    "\n",
    "print(f\"Processing files for state: {state_name} ({state})\")\n",
    "\n",
    "gdf_list = []\n",
    "# Iterate over each .csv.gz file in the directory\n",
    "\n",
    "download_dir = os.path.join(base_dir, 'downloaded_files', state)\n",
    "\n",
    "for filename in tqdm(os.listdir(os.path.join(base_dir, 'downloaded_files', state))):\n",
    "    if filename.endswith('.csv.gz'):\n",
    "        # Join the directory with the filename\n",
    "        file_path = os.path.join(base_dir, 'downloaded_files', state, filename)\n",
    "\n",
    "        # Open the .csv.gz file\n",
    "        with gzip.open(file_path, 'rt') as file:\n",
    "            # Read each line in the .csv.gz file\n",
    "            for line in file:\n",
    "                # Remove leading/trailing whitespace\n",
    "                line = line.strip()\n",
    "\n",
    "                # Skip empty lines\n",
    "                if not line:\n",
    "                        continue\n",
    "\n",
    "                # Parse the line as JSON\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # Create a GeoDataFrame from the data and append it to the list\n",
    "                geometry = shape(data['geometry'])\n",
    "                properties = data['properties']\n",
    "                data_gdf = gpd.GeoDataFrame([properties], geometry=[geometry])\n",
    "                gdf_list.append(data_gdf)\n",
    "\n",
    "# Concatenate all the GeoDataFrames in the list into a single GeoDataFrame\n",
    "final_gdf = pd.concat(gdf_list, ignore_index=True)\n",
    "final_gdf['height'] = final_gdf['height'].astype(float).round(2)\n",
    "# Keep only 'height' attribute and geometry\n",
    "final_gdf = final_gdf[['height', 'geometry']]\n",
    "\n",
    "# Load both GeoDataFrames\n",
    "\n",
    "# Load the shapefile\n",
    "gdf2_path = os.path.join(extract_folder, f\"Deliverable20230728AK\", f\"{state_code}_Structures.gdb\")\n",
    "layers = fiona.listlayers(gdf2_path)\n",
    "print(f\"Available layers: {layers}\")\n",
    "gdf2 = gpd.read_file(gdf2_path, layer=layers[0])\n",
    "    \n",
    "\n",
    "final_gdf = final_gdf.set_crs('EPSG:4326')\n",
    "gdf2 = gdf2.set_crs('EPSG:4326')\n",
    "\n",
    "\n",
    "gdf2['HEIGHT'] = gdf2['HEIGHT'].astype(float).round(2)\n",
    "gdf2 = gdf2[['geometry', 'HEIGHT']]\n",
    "\n",
    "final_gdf['height'] = final_gdf['height'].apply(lambda x: np.nan if x <= 0 else x)\n",
    "gdf2['HEIGHT'] = gdf2['HEIGHT'].apply(lambda x: np.nan if x == 0 else x)\n",
    "\n",
    "# Perform spatial indexing on both GeoDataFrames\n",
    "final_gdf.sindex\n",
    "gdf2.sindex\n",
    "\n",
    "# Perform the spatial join\n",
    "joined = gpd.sjoin(final_gdf, gdf2, how='left', op='intersects')\n",
    "\n",
    "# Calculate 'diff' and 'mean'\n",
    "joined['diff'] = joined.apply(lambda row: row['height'] - row['HEIGHT'] if pd.notnull(row['height']) and pd.notnull(row['HEIGHT']) else np.nan, axis=1)\n",
    "joined['mean'] = joined.apply(lambda row: np.mean([val for val in [row['height'], row['HEIGHT']] if pd.notnull(val)]) if pd.notnull(row['height']) or pd.notnull(row['HEIGHT']) else np.nan, axis=1)\n",
    "\n",
    "joined = joined.drop(columns=['index_right'])\n",
    "\n",
    "joined = joined.to_crs('EPSG:5070')\n",
    "\n",
    "joined['SQMETERS'] = joined['geometry'].area\n",
    "joined['SQMETERS'] = joined['SQMETERS'].round(2)\n",
    "\n",
    "joined = joined.to_crs('EPSG:4326')\n",
    "\n",
    "joined = joined.rename(columns={'height': 'height_MS'})\n",
    "joined = joined.rename(columns={'HEIGHT': 'height_FM'})\n",
    "joined = joined.rename(columns={'diff': 'height_dif'})\n",
    "joined = joined.rename(columns={'mean': 'height_avg'})\n",
    "\n",
    "schema = {'geometry': 'Polygon',\n",
    "'properties': {'height_MS': 'float:10.2',\n",
    "            'height_FM': 'float:10.2',\n",
    "            'height_dif': 'float:10.2',\n",
    "            'height_avg': 'float:10.2',\n",
    "            'SQMETERS': 'float:15.2',}}\n",
    "\n",
    "# Export to GeoJSON\n",
    "print(f\"Saving shp file for state: {state_name} ({state})\")\n",
    "output_filename = os.path.join(base_dir, 'joined', f\"{state}.gpkg\")\n",
    "joined.to_file(output_filename,driver=\"GPKG\",schema=schema)\n",
    "\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
