{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a minimal reproducible example for the state \"AK\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import gzip\n",
    "import json\n",
    "from shapely.geometry import shape\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import zipfile\n",
    "import fiona\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEMA and MS data downloaded and extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Download datasets and needed files\n",
    "\n",
    "def download_and_save(url, dest_path):\n",
    "    \"\"\"Download a file from a given URL and save it locally.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with open(dest_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192): \n",
    "            file.write(chunk)\n",
    "\n",
    "# Extract state code from the ZIP URL\n",
    "zip_url = 'https://disasters.geoplatform.gov/publicdata/Partners/ORNL/USA_Structures/Alaska/Deliverable20230728AK.zip'\n",
    "state_code = zip_url.split('Deliverable')[1][8:10]  # Extracts 'AK' from the provided URL\n",
    "\n",
    "# Define directories\n",
    "base_dir = './downloads'\n",
    "fema_dir = os.path.join(base_dir, 'FEMA')\n",
    "state_folder = os.path.join(fema_dir, state_code)\n",
    "zip_folder = os.path.join(state_folder, 'zip_files')\n",
    "extract_folder = os.path.join(state_folder, 'extracted_files')\n",
    "geojson_folder = os.path.join(base_dir, 'geojson_files')\n",
    "csv_folder = os.path.join(base_dir, 'csv_files')\n",
    "\n",
    "geojson_url = 'https://gist.githubusercontent.com/Feng96/bf831f5cf972d24455a25128ed8b3636/raw/f05772ff636c790f6ebc0e4eb5aa6f7b669f4ea9/data_link.geojsonn'\n",
    "csv_url = 'https://minedbuildings.blob.core.windows.net/global-buildings/dataset-links.csv'\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(zip_folder, exist_ok=True)\n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "os.makedirs(geojson_folder, exist_ok=True)\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "\n",
    "zip_dest_path = os.path.join(zip_folder, f'Deliverable{state_code}.zip')\n",
    "geojson_dest_path = os.path.join(geojson_folder, 'code.geojson')\n",
    "csv_dest_path = os.path.join(csv_folder, 'dataset-links.csv')\n",
    "\n",
    "# Download and save files\n",
    "download_and_save(zip_url, zip_dest_path) # FEMA US Structures ZIP file\n",
    "download_and_save(geojson_url, geojson_dest_path) # MS buildings with height coverage GeoJSON file\n",
    "download_and_save(csv_url, csv_dest_path) # CSV file with links to the MS footprint data\n",
    "\n",
    "# Extract ZIP content\n",
    "with zipfile.ZipFile(zip_dest_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "\n",
    "print('FEMA and MS data downloaded and extracted successfully!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(csv_folder, 'dataset_link.csv'))\n",
    "\n",
    "gdf = gpd.read_file(os.path.join(geojson_folder, 'code.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_url = 'https://raw.githubusercontent.com/giswqs/data/main/us/us_states.csv'\n",
    "states = pd.read_csv(states_url)\n",
    "print(states.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'STUSPS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\utkge\\miniconda3\\envs\\gee\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\utkge\\miniconda3\\envs\\gee\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\utkge\\miniconda3\\envs\\gee\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'STUSPS'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32me:\\building_height\\building_height_code.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/building_height/building_height_code.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m state_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAlsaka\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/building_height/building_height_code.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m state \u001b[39m=\u001b[39m state_code\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/building_height/building_height_code.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m gdf_state \u001b[39m=\u001b[39m gdf[gdf[\u001b[39m'\u001b[39;49m\u001b[39mSTUSPS\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39m state]\u001b[39m.\u001b[39mcopy()  \u001b[39m# Create a copy\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/building_height/building_height_code.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProcessing state: \u001b[39m\u001b[39m{\u001b[39;00mstate_name\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mstate\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/building_height/building_height_code.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Convert 'field_1' to 'QuadKey' as a string\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\utkge\\miniconda3\\envs\\gee\\lib\\site-packages\\geopandas\\geodataframe.py:1415\u001b[0m, in \u001b[0;36mGeoDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m   1410\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1411\u001b[0m \u001b[39m    If the result is a column containing only 'geometry', return a\u001b[39;00m\n\u001b[0;32m   1412\u001b[0m \u001b[39m    GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m \u001b[39m    return a GeoDataFrame.\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1415\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(key)\n\u001b[0;32m   1416\u001b[0m     geo_col \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_geometry_column_name\n\u001b[0;32m   1417\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, Series) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(result\u001b[39m.\u001b[39mdtype, GeometryDtype):\n",
      "File \u001b[1;32mc:\\Users\\utkge\\miniconda3\\envs\\gee\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\utkge\\miniconda3\\envs\\gee\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'STUSPS'"
     ]
    }
   ],
   "source": [
    "#loop through each state\n",
    "state_name = \"Alaska\"\n",
    "state = state_code\n",
    "gdf_state = gdf[gdf['STUSPS'] == state].copy()  # Create a copy\n",
    "\n",
    "print(f\"Processing state: {state_name} ({state})\")\n",
    "\n",
    "# Convert 'field_1' to 'QuadKey' as a string\n",
    "gdf_state.loc[:, 'QuadKey'] = gdf_state['field_1'].apply(lambda x: str(int(str(x)[1:9])))\n",
    "\n",
    "# Ensure 'QuadKey' in df is also a string\n",
    "df['QuadKey'] = df['QuadKey'].astype(str)\n",
    "\n",
    "# Filter out rows where 'QuadKey' does not match with 'QuadKey' in gdf_state\n",
    "df_state = df[df['QuadKey'].isin(gdf_state['QuadKey'])]\n",
    "\n",
    "print(f\"Downloading files for state: {state_name} ({state})\")\n",
    "# Create directory to save downloaded files\n",
    "os.makedirs(os.path.join(base_dir, f'downloaded_files/{state}'), exist_ok=True)\n",
    "\n",
    "# Iterate over all rows in the filtered dataframe\n",
    "for idx, row in tqdm(df_state.iterrows(), total=df_state.shape[0]):\n",
    "        # Get the URL\n",
    "    url = row['Url']\n",
    "\n",
    "    # Make a GET request to the URL\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Get the file name by joining the 'downloaded_files' with the basename of the URL\n",
    "        file_name = os.path.join(base_dir, f'downloaded_files/{state}', f\"{state}_{idx}.csv.gz\")\n",
    "\n",
    "        # Open the file in write mode\n",
    "        with open(file_name, 'wb') as file:\n",
    "            # Write the content of the request to the file\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                file.write(chunk)\n",
    "    else:\n",
    "        print(f\"Failed to download file from url {url}\")\n",
    "\n",
    "print(f\"Processing files for state: {state_name} ({state})\")\n",
    "\n",
    "gdf_list = []\n",
    "# Iterate over each .csv.gz file in the directory\n",
    "\n",
    "download_dir = os.path.join(base_dir, 'downloaded_files', state)\n",
    "\n",
    "for filename in tqdm(os.listdir(os.path.join(base_dir, 'downloaded_files', state))):\n",
    "    if filename.endswith('.csv.gz'):\n",
    "        # Join the directory with the filename\n",
    "        file_path = os.path.join(base_dir, 'downloaded_files', state, filename)\n",
    "\n",
    "        # Open the .csv.gz file\n",
    "        with gzip.open(file_path, 'rt') as file:\n",
    "            # Read each line in the .csv.gz file\n",
    "            for line in file:\n",
    "                # Remove leading/trailing whitespace\n",
    "                line = line.strip()\n",
    "\n",
    "                # Skip empty lines\n",
    "                if not line:\n",
    "                        continue\n",
    "\n",
    "                # Parse the line as JSON\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # Create a GeoDataFrame from the data and append it to the list\n",
    "                geometry = shape(data['geometry'])\n",
    "                properties = data['properties']\n",
    "                data_gdf = gpd.GeoDataFrame([properties], geometry=[geometry])\n",
    "                gdf_list.append(data_gdf)\n",
    "\n",
    "# Concatenate all the GeoDataFrames in the list into a single GeoDataFrame\n",
    "final_gdf = pd.concat(gdf_list, ignore_index=True)\n",
    "final_gdf['height'] = final_gdf['height'].astype(float).round(2)\n",
    "# Keep only 'height' attribute and geometry\n",
    "final_gdf = final_gdf[['height', 'geometry']]\n",
    "\n",
    "# Load both GeoDataFrames\n",
    "\n",
    "# Load the shapefile\n",
    "gdf2_path = os.path.join(extract_folder, f\"Deliverable20230728AK\", f\"{state_code}_Structures.gdb\")\n",
    "layers = fiona.listlayers(gdf2_path)\n",
    "print(f\"Available layers: {layers}\")\n",
    "gdf2 = gpd.read_file(gdf2_path, layer=layers[0])\n",
    "    \n",
    "\n",
    "final_gdf = final_gdf.set_crs('EPSG:4326')\n",
    "gdf2 = gdf2.set_crs('EPSG:4326')\n",
    "\n",
    "\n",
    "gdf2['HEIGHT'] = gdf2['HEIGHT'].astype(float).round(2)\n",
    "gdf2 = gdf2[['geometry', 'HEIGHT']]\n",
    "\n",
    "final_gdf['height'] = final_gdf['height'].apply(lambda x: np.nan if x <= 0 else x)\n",
    "gdf2['HEIGHT'] = gdf2['HEIGHT'].apply(lambda x: np.nan if x == 0 else x)\n",
    "\n",
    "# Perform spatial indexing on both GeoDataFrames\n",
    "final_gdf.sindex\n",
    "gdf2.sindex\n",
    "\n",
    "# Perform the spatial join\n",
    "joined = gpd.sjoin(final_gdf, gdf2, how='left', op='intersects')\n",
    "\n",
    "# Calculate 'diff' and 'mean'\n",
    "joined['diff'] = joined.apply(lambda row: row['height'] - row['HEIGHT'] if pd.notnull(row['height']) and pd.notnull(row['HEIGHT']) else np.nan, axis=1)\n",
    "joined['mean'] = joined.apply(lambda row: np.mean([val for val in [row['height'], row['HEIGHT']] if pd.notnull(val)]) if pd.notnull(row['height']) or pd.notnull(row['HEIGHT']) else np.nan, axis=1)\n",
    "\n",
    "joined = joined.drop(columns=['index_right'])\n",
    "\n",
    "joined = joined.to_crs('EPSG:5070')\n",
    "\n",
    "joined['SQMETERS'] = joined['geometry'].area\n",
    "joined['SQMETERS'] = joined['SQMETERS'].round(2)\n",
    "\n",
    "joined = joined.to_crs('EPSG:4326')\n",
    "\n",
    "joined = joined.rename(columns={'height': 'height_MS'})\n",
    "joined = joined.rename(columns={'HEIGHT': 'height_FM'})\n",
    "joined = joined.rename(columns={'diff': 'height_dif'})\n",
    "joined = joined.rename(columns={'mean': 'height_avg'})\n",
    "\n",
    "schema = {'geometry': 'Polygon',\n",
    "'properties': {'height_MS': 'float:10.2',\n",
    "            'height_FM': 'float:10.2',\n",
    "            'height_dif': 'float:10.2',\n",
    "            'height_avg': 'float:10.2',\n",
    "            'SQMETERS': 'float:15.2',}}\n",
    "\n",
    "# Export to GeoJSON\n",
    "print(f\"Saving shp file for state: {state_name} ({state})\")\n",
    "output_filename = os.path.join(base_dir, 'joined', f\"{state}.gpkg\")\n",
    "joined.to_file(output_filename,driver=\"GPKG\",schema=schema)\n",
    "\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
